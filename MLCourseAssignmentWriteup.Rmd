Practical Machine Learning - Course Assignment
========================================================

This document describe the analysis done for the prediction assignment of the practical machine learning course.

The first part is the declaration of the package which will be used. 
Note : to be reproductible, I also set the seed value.

```{r}
options(warn=-1)
library(caret)
library(randomForest)
library(Hmisc)

library(foreach)
library(doParallel)
set.seed(4356)
```

The first step is to load the csv file data to dataframe and analyze the type & the completion rate of the data:

```{r}
data <- read.csv("pml-training.csv")
#summary(data)
#describe(data)
#sapply(data, class)
#str(data)
```

This analysis allows us to note two main points :
 1 - Some numeric data have been imported as factor because of the presence of some characters ("#DIV/0!")
 2 - Some columns have a really low completion rate (a lot of missing data)
 
To manage the first issue we need to reimport data ignoring "#DIV/0!" values :

```{r}
data <- read.csv("pml-training.csv", na.strings=c("#DIV/0!") )
```

And force the cast to numeric values for the specified columns (i.e.: 8 to end) :

```{r}
cData <- data
for(i in c(8:ncol(cData)-1)) {cData[,i] = as.numeric(as.character(cData[,i]))}
```

To manage the second issue we will select as feature only the column with a 100% completion rate. We will also filter some features which seem to be useless like "X"", timestamps, "new_window" and "num_window". 

```{r}
featuresnames <- colnames(cData[colSums(is.na(cData)) == 0])[-(1:7)]
features <- cData[featuresnames]
```


We have now a dataframe features which contains all the workable features. The next step is to split the dataset in two part : training and testing.

```{r}
xdata <- createDataPartition(y=features$classe, p=3/4, list=FALSE )
training <- features[xdata,]
testing <- features[-xdata,]
```


We can now train a classifier with the training data. We ask to process 4 random forest with 150 trees each and combine then to have a random forest model with a total of 600 trees.
```{r}
registerDoParallel()
model <- foreach(ntree=rep(150, 4), .combine=randomForest::combine) %dopar% randomForest(training[-ncol(training)], training$classe, ntree=ntree)
```

To evaluate the model we will use the confusionmatrix method and we will focus on accuracy, sensitivity & specificity metrics :
```{r}
predictionsTr <- predict(model, newdata=training)
confusionMatrix(predictionsTr,training$classe)


predictionsTe <- predict(model, newdata=testing)
confusionMatrix(predictionsTe,testing$classe)
```

As seen by the result of the confusionmatrix, the model is good and efficient because it has an accuracy of 0.997 and very good sensitivity & specificity values on the testing dataset. (the lowest value is 0.992 for the sensitivity of the class C)